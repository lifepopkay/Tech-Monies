{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lifepopkay/Tech-Monies/blob/main/Main_dataScrappingIndeedScript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAfvg3qt12M5"
   },
   "source": [
    "### Data Dictionary\n",
    "#### Main Columns:\n",
    "---\n",
    "| Information | Dataset Column | Available | Comment |\n",
    "|---|---|---|---|\n",
    "| Jobs title | `title` | ‚úÖ | Posted Job Title |\n",
    "| Description | `jobDesc` | ‚úÖ | All details available in JD. Use `print` statement to get a formatted output |\n",
    "| Salary | `salary` | ‚ùå | will be extracted from `salaryDesc` |\n",
    "| Contract Type | `type` | ‚úÖ | will be extracted from `salaryDesc` |\n",
    "| Company Name | `company` | ‚ùå | - |\n",
    "| Country | `country` | ‚ùå | will be extracted from `location` |\n",
    "| State | `state` | ‚ùå | will be extracted from `location` |\n",
    "| Years of Experience | `yearMinExp` | ‚ùå | will be extracted from `jobDesc` |\n",
    "| Position | `level` | ‚ùå | will be extracted from `jobDesc` |\n",
    "| Industry | `industry` | ‚ùå | will be extracted from `jobDesc` |\n",
    "| Age Required | `ageCriteria` | ‚ùå | will be extracted from `jobDesc` |\n",
    "| Skillset Required | `skills` | ‚ùå | will be extracted from `jobDesc` |\n",
    "| Educational qualification | `eligibility` | ‚ùå | will be extracted from `jobDesc` | \n",
    "| Pay Frequency | `payFrequency` | ‚ùå | will be extracted from `jobDesc` |\n",
    "\n",
    "---\n",
    "\n",
    "There are some more columns available which are listed below.\n",
    "\n",
    "#### Additional Columns:\n",
    "\n",
    "| Information | Dataset Column | Available | Comment |\n",
    "|---|---|---|---|\n",
    "| Jobs ID | `jobID` | ‚úÖ | - |\n",
    "| Location | `location` | ‚úÖ | One or more combination of city, state, country or pincode/zipcode |\n",
    "| Salary Desc | `salaryDesc` | ‚úÖ | One or more combination of salary (actual/estimated), job type, shift, etc. |\n",
    "| JD link | `link` | ‚úÖ | Link to actual Job Description provided by Indeed |\n",
    "| Post Date | `postDate` | ‚úÖ | Recency of Job Posting |\n",
    "| Estimated by Indeed | `estimated` | ‚ùå | The salary is estimated by Indeed |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBi6qCqKeInk"
   },
   "source": [
    "### Execute the block\n",
    "**Instructions:**\n",
    "\n",
    "1. Enter Job Title. üî¥\n",
    "2. Enter Country Abbreviations - üî¥\n",
    "\n",
    "| Country | Base Url |\n",
    "|---|---|\n",
    "| **USA** | `www.indeed.com` |\n",
    "| **UK** | `uk.indeed.com` |\n",
    "| **IND** | `in.indeed.com` |\n",
    "| **NG** | `ng.indeed.com` |\n",
    "| **CA** | `ca.indeed.com` |\n",
    "\n",
    "3. Enter location. This could be any city, state or province. Keep blank & Hit Enter/Return (‚Ü©) to get result across country. üü¢\n",
    "4. Enter Page Numbers to be scrapped. Keep blank & Hit Enter/Return (‚Ü©) to get result from 1st page only. üü¢\n",
    "\n",
    "##### üî¥ - Necessary Inputs, üü¢ - Optional Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "id": "pRAs6pR9PLmV",
    "outputId": "2933013d-1853-40e0-ac55-8633f5f0306b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter job title: data analyst\n",
      "Enter country code: UK\n",
      "Enter job location: \n",
      "Pages to Scrap: 600\n",
      "===========| Start |============\n",
      "===| Scrapping Job Postings |===\n",
      "\n",
      "+++++ Extracting Data from Page 1 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date\n",
      "Found Total 15 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 2 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=10\n",
      "Found Total 30 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 3 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=20\n",
      "Found Total 45 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 4 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=30\n",
      "Found Total 60 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 5 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=40\n",
      "Found Total 75 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 6 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=50\n",
      "Found Total 90 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 7 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=60\n",
      "Found Total 105 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 8 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=70\n",
      "Found Total 120 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 9 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=80\n",
      "Found Total 135 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 10 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=90\n",
      "Found Total 150 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 11 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=100\n",
      "Found Total 165 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 12 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=110\n",
      "Found Total 180 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 13 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=120\n",
      "Found Total 195 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 14 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=130\n",
      "Found Total 210 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 15 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=140\n",
      "Found Total 225 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 16 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=150\n",
      "Found Total 240 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 17 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=160\n",
      "Found Total 255 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 18 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=170\n",
      "Found Total 270 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 19 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=180\n",
      "Found Total 285 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 20 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=190\n",
      "Found Total 300 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 21 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=200\n",
      "Found Total 315 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 22 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=210\n",
      "Found Total 330 jobs so far.\n",
      "\n",
      "+++++ Extracting Data from Page 23 ++++++\n",
      "Extracting data from URL: https://uk.indeed.com/jobs?q=data%20analyst&l=&sort=date&start=220\n"
     ]
    }
   ],
   "source": [
    "#@title Imports and Functions { display-mode: \"form\" }\n",
    "### Imports\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Web Element Manipulation\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# timestamping\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "#Import the packages \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# define headers for connection string\n",
    "headers = {\"User-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"}\n",
    "\n",
    "### Functions\n",
    "old_page = ''\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"prefs\", {\"profile.default_content_settings.cookies\": 2})\n",
    "\n",
    "DRIVER_PATH = 'C:\\Program Files\\Google\\Chrome\\Application\\chromedriver.exe' # ensure to change this to your chrome driver location \n",
    "driver = webdriver.Chrome(executable_path = DRIVER_PATH, chrome_options=chrome_options)\n",
    "\n",
    "def openpage(url):\n",
    "    global old_page\n",
    "\n",
    "    driver.get(url)\n",
    "    #ime.sleep(10)\n",
    "    cont = driver.find_elements(By.CLASS_NAME, 'is-desktop')\n",
    "    try:\n",
    "        element = cont[0].get_attribute('outerHTML')\n",
    "        page = bs(element, \"html.parser\")\n",
    "        old_page = page\n",
    "    except:\n",
    "        page = old_page \n",
    "        \n",
    "    #driver.close()\n",
    "    return page\n",
    "\n",
    "\n",
    "def find_jobs(what, where, baseUrl, nPage=1, verbose=True):\n",
    "    # Handling URL\n",
    "    jobTitle = '%20'.join(what.split())\n",
    "    location = '%20'.join(where.split())\n",
    "\n",
    "    # Initialize list for all jobs data\n",
    "    jobs = []\n",
    "\n",
    "    # Initialize stopping criteria\n",
    "    totalJobs = -1\n",
    "    mPage = nPage\n",
    "\n",
    "    # Initialize Page Index\n",
    "    page = 0\n",
    "\n",
    "    # starting\n",
    "    if verbose:\n",
    "        print('===========| Start |============')\n",
    "        print('===| Scrapping Job Postings |===')\n",
    "\n",
    "    # create url for scraping\n",
    "    while page < min(nPage, mPage):\n",
    "        if page>0:\n",
    "            targetUrl = baseUrl+\"/jobs?q=\"+jobTitle+\"&l=\"+location+\"&sort=date\"+\"&start=\"+str(page * 10)\n",
    "        else:\n",
    "            targetUrl = baseUrl+\"/jobs?q=\"+jobTitle+\"&l=\"+location+\"&sort=date\"\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n+++++ Extracting Data from Page\", page+1, \"++++++\")\n",
    "            print(\"Extracting data from URL:\", targetUrl)\n",
    "        jobs, totalJobs, mPage = scrap_jobs(jobs, targetUrl, verbose, totalJobs, mPage)\n",
    "        page += 1\n",
    "\n",
    "    # attach JD\n",
    "    if verbose:\n",
    "        print('\\n===| Attaching Job Description |===')\n",
    "\n",
    "    # attach Job Description\n",
    "    attach_jd(jobs, baseUrl)\n",
    "\n",
    "    # drop duplicates\n",
    "    jobsDF = pd.DataFrame(jobs)\n",
    "    jobsDF.drop_duplicates(inplace=True)\n",
    "\n",
    "    # extract & add other columns\n",
    "    # add_cols(jobsDF)\n",
    "\n",
    "    # finishing\n",
    "    if verbose:\n",
    "        print('\\n===| Cleaning Up |===')\n",
    "        print(\"Total\", jobsDF.shape[0], \"unique jobs found.\")\n",
    "        print('\\n========| Done |=========')\n",
    "\n",
    "    # Check scrapped jobs\n",
    "    return jobsDF\n",
    "\n",
    "\n",
    "def scrap_jobs(jobs, url, verbose, totalJobs, mPage):\n",
    "\n",
    "    # get the static page to scrap everything apart from job description\n",
    "    \"\"\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if verbose:\n",
    "        if response.ok:\n",
    "            print(\"Connected to\", url, \"Successfully.\")\n",
    "        else:\n",
    "            print(\"Connection denied with response code:\", response.status_code)\n",
    "    ##html = response.text \"\"\"\n",
    "    # Create soup\n",
    "    #soup = BeautifulSoup(html, 'html.parser')\n",
    "    soup = openpage(url)\n",
    "\n",
    "    # Get Actual value for max Page\n",
    "    if totalJobs == -1:\n",
    "        if soup.find('div', {'id': 'searchCountPages'}) is None:\n",
    "            totalJobs = 0\n",
    "        else:\n",
    "            totalJobs = int(re.search(r'of (.*) jobs',  soup.find('div', {'id': 'searchCountPages'}).text)[1].replace(\",\", \"\"))\n",
    "            # Estimate Actual Page number\n",
    "            mPage = (totalJobs//15) + 1\n",
    "\n",
    "    # Search Area\n",
    "    block=soup.find('ul',attrs={'class': re.compile('jobsearch-ResultsList')})\n",
    "\n",
    "    # Check for stopping criteria\n",
    "    #jobCards = block.find_all('div', {'class': 'job_seen_beacon'})\n",
    "    # if len(jobCards) < 15:\n",
    "    #   proceed = 0\n",
    "\n",
    "    # iterate through job cards\n",
    "    for card in block.find_all('div', {'class': 'job_seen_beacon'}):\n",
    "        jobs.append(scrap_cards(card))\n",
    "    if verbose:\n",
    "        print(\"Found Total\", len(jobs), \"jobs so far.\")\n",
    "        \n",
    "    return jobs, totalJobs, mPage\n",
    "\n",
    "def scrap_cards(card):\n",
    "    # temporary dictionary\n",
    "    tempDict = dict()\n",
    "\n",
    "    # Job Title & ID:\n",
    "    title = card.find('h2',{'class':re.compile('jobTitle')})\n",
    "    if not(isinstance(title, type(None))):\n",
    "        tempDict['title']=title.find('a').text\n",
    "        tempDict['id']=title.find('a').attrs['id']\n",
    "\n",
    "    # Company Name:\n",
    "    company = card.find('span',{'class':'companyName'})\n",
    "    if not(isinstance(company, type(None))):\n",
    "        tempDict['company']=company.text\n",
    "\n",
    "    # Location:\n",
    "    location = card.find('div',{'class':'companyLocation'})\n",
    "    if not(isinstance(location, type(None))):\n",
    "        tempDict['location']=location.text\n",
    "\n",
    "    # Links: these Href links will take us to full job description\n",
    "    link = card.find('a', {'class': re.compile('jcs-JobTitle')})\n",
    "    if not(isinstance(link, type(None))):\n",
    "        tempDict['link']=link['href']\n",
    "\n",
    "    # Salary & Contract Type, if available:'\n",
    "    # picking all text, cleaning will be done later\n",
    "    salaryCard = card.find('div',{'class': re.compile('metadataContainer')})\n",
    "    if not(isinstance(salaryCard, type(None))):\n",
    "        tempDict['salaryDesc'] = salaryCard.text\n",
    "      # salary = salaryCard.find('div',{'class': re.compile('salary')})\n",
    "      # if not(isinstance(salary, type(None))):\n",
    "      #   tempDict['salary']=salary.text\n",
    "      # contract = salaryCard.find('div',{'class': 'metadata'})\n",
    "      # if not(isinstance(contract, type(None))):\n",
    "      #   tempDict['contractType']=contract.text\n",
    "\n",
    "    # Job Post Date:\n",
    "    postDate = card.find('span', attrs={'class': 'date'})\n",
    "    if not(isinstance(postDate, type(None))):\n",
    "        tempDict['postDate']=postDate.find(text=True, recursive=False)\n",
    "\n",
    "    # Contract Type:\n",
    "    # contractType = card.find('div', attrs={'class': 'attribute_snippet'})\n",
    "    # if not(isinstance(contractType, type(None))):\n",
    "    #   tempDict['contractType']=contractType.text\n",
    "\n",
    "    # Put everything together in a list of lists for the default dictionary\n",
    "    return tempDict\n",
    "\n",
    "def attach_jd(jobs, baseUrl):\n",
    "    \n",
    "    for dict in jobs:\n",
    "        #response = requests.get(baseUrl+dict['link'], headers=headers)\n",
    "        pager = openpage(baseUrl+dict['link'])\n",
    "        try:\n",
    "            soup_ = pager.find('div',{'class':'jobsearch-jobDescriptionText'}).text\n",
    "            if not(isinstance(soup_, type(None))):\n",
    "                dict['JobDesc'] = soup_\n",
    "        except:\n",
    "            dict['JobDesc'] = 'Not Available'\n",
    "        \"\"\"\n",
    "        if response.ok:\n",
    "          html_ = response.text\n",
    "          # Create soup\n",
    "          soup_ = BeautifulSoup(html_, 'html.parser').find('div',{'class':'jobsearch-jobDescriptionText'}).text\n",
    "          if not(isinstance(soup_, type(None))):\n",
    "            dict['JobDesc'] = soup_\n",
    "        else:\n",
    "          dict['JobDesc'] = 'Not Available' \"\"\"\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "# def add_cols(jobs):\n",
    "#   # Job Type:\n",
    "#   def job_type(x):\n",
    "#     return 'Contract' if x.find('Contract') != -1 else 'FullTime' if x.find('Full-time') != -1 else None\n",
    "#   # Salary Range:\n",
    "#   def salary_range(x):\n",
    "#     return 'Contract' if x.find('Contract') != -1 else 'FullTime' if x.find('Full-time') != -1 else None\n",
    "#   # Estimated Salary:\n",
    "#   def estimated(x):\n",
    "#     return 1 if x.find('Estimated') != -1 else 0\n",
    "  \n",
    "#   # add Salary related columns\n",
    "#   jobs['contractType'] = jobs.salaryDesc.apply(job_type)\n",
    "#   jobs['estimated'] = jobs.salaryDesc.apply(estimated)\n",
    "\n",
    "#   return jobs\n",
    "\n",
    "##### Main Code Section\n",
    "# avaialable domain\n",
    "domains = {\n",
    "    'usa': 'www.indeed.com',\n",
    "    'uk': 'uk.indeed.com',\n",
    "    'ind': 'in.indeed.com',\n",
    "    'ng': 'ng.indeed.com',\n",
    "    'ca': 'ca.indeed.com'\n",
    "}\n",
    "## Inputs from Users\n",
    "what = input('Enter job title: ')\n",
    "country = input('Enter country code: ')\n",
    "where = input('Enter job location: ')\n",
    "nPage = input('Pages to Scrap: ')\n",
    "# dtransformations for functions\n",
    "baseUrl = 'https://'+domains[country.lower()]\n",
    "nPage = 1 if not nPage else int(nPage)\n",
    "\n",
    "# Scrap Data\n",
    "df = find_jobs(what, where, baseUrl, nPage)\n",
    "\n",
    "# Enter Job Title, Location/Country, primary URL, total pages for extraction & if any \n",
    "# data = find_jobs(what='Refuse Collector', where='USA', baseUrl='https://www.indeed.com', nPage=10, verbose=True)\n",
    "\n",
    "## Write to file\n",
    "fileName = what.replace(' ', '_')+'_'+where.replace(' ', '_')+'_'+country.upper()+'_'+str(date.today()).replace('-', '')+'.csv'\n",
    "print('\\nWriting to file:', fileName)\n",
    "df.to_csv(fileName,index=False)\n",
    "print('\\nDone. Please find file:', fileName, 'in left pane. Refresh, if required.')\n",
    "\n",
    "## Print Outputs\n",
    "print('\\n===| Showing 10 records |===\\n')\n",
    "display(df[['title', 'company', 'location', 'salaryDesc', 'postDate']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYhXK_1aZx4p"
   },
   "source": [
    "### Change Log\n",
    "Track of changes done on this notebook -\n",
    "\n",
    "| Date | Type | User | Details |\n",
    "|---|---|---|---|\n",
    "| 2022-08-13 | New Notebook | `@ajmasih0309` | Setting up basic functionalities to scrape data from job cards & corresponding job descriptions from indeed USA & other countries. Basic Cleaning of data to get useful output |\n",
    "| 2022-08-14 | Modified Notebook | `@ajmasih0309` | Basic Cleaning of data to get useful output & visual aid for users who will execute the script for extraction |\n",
    "| 2022-08-15 | Modified Notebook | `@ajmasih0309` | Added some cleaning steps |\n",
    "| 2022-08-16 | Modified Notebook | `@ajmasih0309` | Removed cleaning steps. Tested multiple domain & added functionality to select country using abbreviations & select for location in country. |\n",
    "| 2022-08-17 | Modified Notebook | `@ajmasih0309` | Modified Total Jobs checking technique & Job Descriptions to avoid failure. |\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HID8kcT9pdAj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOFh1R9pH6BqBnS0lN6lXoT",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Main_dataScrappingIndeedScript.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
